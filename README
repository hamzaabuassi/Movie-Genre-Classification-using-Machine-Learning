# üé¨ Movie Genre Classification Using Machine Learning

This project focuses on classifying movie genres based on movie descriptions and metadata using classical machine learning techniques combined with TF-IDF text vectorization.

---

## üöÄ Project Overview

- **Dataset:** Movie metadata including `Description`, `Genre`, `Rating`, `Duration`, and `Release Year`.
- **Text Processing:** TF-IDF vectorization of movie descriptions to convert text into numerical features.
- **Data Balancing:** Addressed class imbalance using SMOTE, Random OverSampler, and Random UnderSampler.
- **Feature Scaling:** Applied StandardScaler, MinMaxScaler, and RobustScaler.
- **Dimensionality Reduction:** Used PCA, LDA, and TruncatedSVD to reduce feature space.
- **Models Used:** Logistic Regression, Decision Tree, Random Forest, SVM, KNN, XGBoost, Naive Bayes, Gradient Boosting, MLP, Extra Trees.
- **Evaluation Metrics:** Accuracy, Precision, Recall, F1-Score.
- **Visualization:** Word clouds, PCA scatterplots, heatmaps, bar plots, and interactive visualizations with Plotly.

---

## üìÅ Dataset

- **Source:** Kaggle  
- **File:** `movie_genre_classification_final.csv`  
- **Description:** Contains movie descriptions, genres, and numerical features like rating, duration, and release year.

---

## üß™ Methodology

### 1. Data Loading and Cleaning

- Loaded dataset and converted `Description` column to string type.
- Imputed missing values using the most frequent value strategy.

### 2. Exploratory Data Analysis (EDA)

- Plotted genre distribution (log scale).
- Generated word clouds per genre to visualize frequent words.
- Applied TF-IDF vectorization on descriptions.
- Visualized TF-IDF features using PCA.
- Identified top informative words via mutual information.
- Visualized numerical features by genre using violin plots.

### 3. Feature Engineering and Preprocessing

- Encoded categorical features using Label Encoding.
- Balanced dataset with different resampling techniques.
- Scaled features with various scalers.
- Reduced dimensionality to improve model performance and efficiency.

### 4. Model Training and Evaluation

- Trained 10 different classifiers over multiple combinations of preprocessing steps.
- Evaluated models using accuracy, precision, recall, and F1-score.
- Stored all results for analysis.

### 5. Result Visualization

- Bar plots showing top models by F1-score.
- Heatmaps illustrating the effect of scalers and reducers on model performance.
- Interactive plots for detailed performance exploration.

---

## üìä Best Model Results (Sample of Top Runs)

| Encoding       | Balancer            | Scaler         | Reducer     | Model            | Accuracy | Precision | Recall | F1-Score |
|----------------|---------------------|----------------|-------------|------------------|----------|-----------|--------|----------|
| LabelEncoding  | RandomUnderSampler   | MinMaxScaler   | PCA         | MLP              | 1.0      | 1.0       | 1.0    | 1.0      |
| LabelEncoding  | SMOTE               | MinMaxScaler   | PCA         | SVM              | 1.0      | 1.0       | 1.0    | 1.0      |
| LabelEncoding  | SMOTE               | MinMaxScaler   | LDA         | RandomForest     | 1.0      | 1.0       | 1.0    | 1.0      |
| LabelEncoding  | NoBalance           | MinMaxScaler   | LDA         | ExtraTree        | 1.0      | 1.0       | 1.0    | 1.0      |

*(This is a sample; the project generated over 360 results from various combinations.)*

---

## üì¶ Installation

```bash
pip install pandas numpy matplotlib seaborn plotly scikit-learn xgboost imbalanced-learn wordcloud
